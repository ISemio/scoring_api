Buro bal shape :  (27299925, 3)
transform to dummies
Counting buros
averaging buro bal
Read Bureau
Go to dummies
Merge with buro avg
Counting buro per SK_ID_CURR
Averaging bureau
            SK_ID_BUREAU  DAYS_CREDIT  ...  avg_buro_buro_bal_status_X  avg_buro_buro_countSK_ID_CURR                             ...
100001               7.0  -735.000000  ...                    0.214590            24.571429100002               8.0  -874.000000  ...                    0.161932            13.750000100003               4.0 -1400.750000  ...                         NaN                  NaN100004               2.0  -867.000000  ...                         NaN                  NaN100005               3.0  -190.666667  ...                    0.136752             7.000000
[5 rows x 46 columns]
Read prev
Go to dummies
Counting number of Prevs
Averaging prev
            SK_ID_PREV  ...  PRODUCT_COMBINATION_POS others without interest
SK_ID_CURR              ...
100001             1.0  ...                                              0.0
100002             1.0  ...                                              0.0
100003             3.0  ...                                              0.0
100004             1.0  ...                                              0.0
100005             2.0  ...                                              0.0

[5 rows x 163 columns]
Reading POS_CASH
Go to dummies
Compute nb of prevs per curr
Go to averages
Reading CC balance
Go to dummies
Compute average
Reading Installments
Read data
Shape data:  (307511, 122)
Shape data:  (307511, 121)
missing_cols 49
Shape data for dashboard:  (307511, 6)
Shape data:  (307511, 331)
cols_2_keep 331
        SK_ID_CURR  NAME_CONTRACT_TYPE  ...  inst_AMT_INSTALMENT  inst_AMT_PAYMENT
0           100002                   0  ...         11559.247105      11559.247105
1           100003                   0  ...         64754.586000      64754.586000
2           100004                   1  ...          7096.155000       7096.155000
3           100006                   0  ...         62947.088438      62947.088438
4           100007                   0  ...         12666.444545      12214.060227
...            ...                 ...  ...                  ...               ...
307506      456251                   0  ...          7492.924286       7492.924286
307507      456252                   0  ...         10069.867500      10069.867500
307508      456253                   0  ...          4399.707857       4115.915357
307509      456254                   0  ...         10239.832895      10239.832895
307510      456255                   0  ...         41464.713649      47646.215878

[246008 rows x 331 columns]
0         1
1         0
2         0
3         0
4         0
         ..
307506    0
307507    0
307508    0
307509    1
307510    0
Name: TARGET, Length: 246008, dtype: int64
Counter({0: 226148, 1: 19860})
Counter({0: 226148, 1: 45229})
Counter({0: 64612, 1: 45229})
[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current 
value: num_threads=-1
Training until validation scores don't improve for 100 rounds
[100]   training's binary_logloss: 0.559017     valid_1's binary_logloss: 0.475936
[200]   training's binary_logloss: 0.527746     valid_1's binary_logloss: 0.459544
[300]   training's binary_logloss: 0.507934     valid_1's binary_logloss: 0.451566
[400]   training's binary_logloss: 0.492308     valid_1's binary_logloss: 0.446238
[500]   training's binary_logloss: 0.478733     valid_1's binary_logloss: 0.441946
[600]   training's binary_logloss: 0.466842     valid_1's binary_logloss: 0.438316
[700]   training's binary_logloss: 0.455857     valid_1's binary_logloss: 0.435089
[800]   training's binary_logloss: 0.444963     valid_1's binary_logloss: 0.431822
[900]   training's binary_logloss: 0.43478      valid_1's binary_logloss: 0.42894
[1000]  training's binary_logloss: 0.424966     valid_1's binary_logloss: 0.426079
[1100]  training's binary_logloss: 0.415896     valid_1's binary_logloss: 0.423427
[1200]  training's binary_logloss: 0.4067       valid_1's binary_logloss: 0.420724
[1300]  training's binary_logloss: 0.398067     valid_1's binary_logloss: 0.41822
[1400]  training's binary_logloss: 0.389821     valid_1's binary_logloss: 0.4158
[1500]  training's binary_logloss: 0.382337     valid_1's binary_logloss: 0.41368
[1600]  training's binary_logloss: 0.374326     valid_1's binary_logloss: 0.411494
[1700]  training's binary_logloss: 0.366774     valid_1's binary_logloss: 0.409443
[1800]  training's binary_logloss: 0.359364     valid_1's binary_logloss: 0.407334
[1900]  training's binary_logloss: 0.352309     valid_1's binary_logloss: 0.405416
[2000]  training's binary_logloss: 0.346072     valid_1's binary_logloss: 0.403782
[2100]  training's binary_logloss: 0.339184     valid_1's binary_logloss: 0.4019
[2200]  training's binary_logloss: 0.332983     valid_1's binary_logloss: 0.400121
[2300]  training's binary_logloss: 0.326638     valid_1's binary_logloss: 0.398324
[2400]  training's binary_logloss: 0.320685     valid_1's binary_logloss: 0.39659
[2500]  training's binary_logloss: 0.314904     valid_1's binary_logloss: 0.394966
[2600]  training's binary_logloss: 0.308843     valid_1's binary_logloss: 0.39332
[2700]  training's binary_logloss: 0.303377     valid_1's binary_logloss: 0.391863
[2800]  training's binary_logloss: 0.298042     valid_1's binary_logloss: 0.390343
[2900]  training's binary_logloss: 0.292555     valid_1's binary_logloss: 0.388715
[3000]  training's binary_logloss: 0.287383     valid_1's binary_logloss: 0.387391
[3100]  training's binary_logloss: 0.282279     valid_1's binary_logloss: 0.385949
[3200]  training's binary_logloss: 0.277834     valid_1's binary_logloss: 0.38482
[3300]  training's binary_logloss: 0.272931     valid_1's binary_logloss: 0.383487
[3400]  training's binary_logloss: 0.268765     valid_1's binary_logloss: 0.38248
[3500]  training's binary_logloss: 0.264183     valid_1's binary_logloss: 0.381222
[3600]  training's binary_logloss: 0.259745     valid_1's binary_logloss: 0.379976
[3700]  training's binary_logloss: 0.255409     valid_1's binary_logloss: 0.378854
[3800]  training's binary_logloss: 0.250913     valid_1's binary_logloss: 0.377607
[3900]  training's binary_logloss: 0.246606     valid_1's binary_logloss: 0.376528
[4000]  training's binary_logloss: 0.242651     valid_1's binary_logloss: 0.375383
[4100]  training's binary_logloss: 0.238753     valid_1's binary_logloss: 0.374334
[4200]  training's binary_logloss: 0.235109     valid_1's binary_logloss: 0.373414
[4300]  training's binary_logloss: 0.231269     valid_1's binary_logloss: 0.372444
[4400]  training's binary_logloss: 0.227307     valid_1's binary_logloss: 0.371334
[4500]  training's binary_logloss: 0.223779     valid_1's binary_logloss: 0.37043
[4600]  training's binary_logloss: 0.220307     valid_1's binary_logloss: 0.369576
[4700]  training's binary_logloss: 0.216917     valid_1's binary_logloss: 0.368743
[4800]  training's binary_logloss: 0.213528     valid_1's binary_logloss: 0.367891
[4900]  training's binary_logloss: 0.210264     valid_1's binary_logloss: 0.367021
[5000]  training's binary_logloss: 0.207065     valid_1's binary_logloss: 0.366217
[5100]  training's binary_logloss: 0.203886     valid_1's binary_logloss: 0.365465
[5200]  training's binary_logloss: 0.20071      valid_1's binary_logloss: 0.36471
[5300]  training's binary_logloss: 0.197597     valid_1's binary_logloss: 0.363898
[5400]  training's binary_logloss: 0.194571     valid_1's binary_logloss: 0.363256
[5500]  training's binary_logloss: 0.191548     valid_1's binary_logloss: 0.362548
[5600]  training's binary_logloss: 0.188618     valid_1's binary_logloss: 0.361759
[5700]  training's binary_logloss: 0.185795     valid_1's binary_logloss: 0.361222
[5800]  training's binary_logloss: 0.183022     valid_1's binary_logloss: 0.360574
[5900]  training's binary_logloss: 0.180154     valid_1's binary_logloss: 0.35977
[6000]  training's binary_logloss: 0.177539     valid_1's binary_logloss: 0.359171
[6100]  training's binary_logloss: 0.175044     valid_1's binary_logloss: 0.358604
[6200]  training's binary_logloss: 0.172444     valid_1's binary_logloss: 0.358039
[6300]  training's binary_logloss: 0.169916     valid_1's binary_logloss: 0.357364
[6400]  training's binary_logloss: 0.167378     valid_1's binary_logloss: 0.356795
[6500]  training's binary_logloss: 0.164932     valid_1's binary_logloss: 0.356229
[6600]  training's binary_logloss: 0.162387     valid_1's binary_logloss: 0.355576
[6700]  training's binary_logloss: 0.160064     valid_1's binary_logloss: 0.355063
[6800]  training's binary_logloss: 0.157759     valid_1's binary_logloss: 0.354629
[6900]  training's binary_logloss: 0.155605     valid_1's binary_logloss: 0.354179
[7000]  training's binary_logloss: 0.153442     valid_1's binary_logloss: 0.353763
[7100]  training's binary_logloss: 0.151419     valid_1's binary_logloss: 0.353352
[7200]  training's binary_logloss: 0.149376     valid_1's binary_logloss: 0.353001
[7300]  training's binary_logloss: 0.147387     valid_1's binary_logloss: 0.352597
[7400]  training's binary_logloss: 0.145219     valid_1's binary_logloss: 0.352159
[7500]  training's binary_logloss: 0.143177     valid_1's binary_logloss: 0.351681
[7600]  training's binary_logloss: 0.141163     valid_1's binary_logloss: 0.351291
[7700]  training's binary_logloss: 0.139249     valid_1's binary_logloss: 0.35096
[7800]  training's binary_logloss: 0.13744      valid_1's binary_logloss: 0.350606
[7900]  training's binary_logloss: 0.135622     valid_1's binary_logloss: 0.35026
[8000]  training's binary_logloss: 0.133764     valid_1's binary_logloss: 0.349962
[8100]  training's binary_logloss: 0.131963     valid_1's binary_logloss: 0.349666
[8200]  training's binary_logloss: 0.130324     valid_1's binary_logloss: 0.349382
[8300]  training's binary_logloss: 0.128628     valid_1's binary_logloss: 0.349078
[8400]  training's binary_logloss: 0.126815     valid_1's binary_logloss: 0.348656
[8500]  training's binary_logloss: 0.12525      valid_1's binary_logloss: 0.348374
[8600]  training's binary_logloss: 0.123706     valid_1's binary_logloss: 0.34817
[8700]  training's binary_logloss: 0.122105     valid_1's binary_logloss: 0.347933
[8800]  training's binary_logloss: 0.120427     valid_1's binary_logloss: 0.347647
[8900]  training's binary_logloss: 0.118861     valid_1's binary_logloss: 0.347458
[9000]  training's binary_logloss: 0.117301     valid_1's binary_logloss: 0.347258
[9100]  training's binary_logloss: 0.115676     valid_1's binary_logloss: 0.347006
[9200]  training's binary_logloss: 0.114211     valid_1's binary_logloss: 0.346819
[9300]  training's binary_logloss: 0.112717     valid_1's binary_logloss: 0.346638
[9400]  training's binary_logloss: 0.111181     valid_1's binary_logloss: 0.346396
[9500]  training's binary_logloss: 0.109712     valid_1's binary_logloss: 0.346158
[9600]  training's binary_logloss: 0.108365     valid_1's binary_logloss: 0.346029
[9700]  training's binary_logloss: 0.106994     valid_1's binary_logloss: 0.345884
[9800]  training's binary_logloss: 0.105628     valid_1's binary_logloss: 0.345667
[9900]  training's binary_logloss: 0.104262     valid_1's binary_logloss: 0.34553
[10000] training's binary_logloss: 0.103007     valid_1's binary_logloss: 0.345446
Did not meet early stopping. Best iteration is:
[10000] training's binary_logloss: 0.103007     valid_1's binary_logloss: 0.345446
Fold  1 AUC : 0.756816
(307511,)
Accuracy: 0.8553403898996796
F1 score: 0.30746477776912895
Recall: 0.39778449144008055
Precision: 0.25057092108601875
>Train: 0=189399, 1=16633, Test: 0=93287, 1=8192
(101479,)
Accuracy: 0.9128785265917086
F1 score: 0.6164258753091241
Recall: 0.8671875
Precision: 0.47815844383119066
        SK_ID_CURR  NAME_CONTRACT_TYPE  ...  inst_AMT_INSTALMENT  inst_AMT_PAYMENT
0           100002                   0  ...         11559.247105      11559.247105
1           100003                   0  ...         64754.586000      64754.586000
2           100004                   1  ...          7096.155000       7096.155000
3           100006                   0  ...         62947.088438      62947.088438
4           100007                   0  ...         12666.444545      12214.060227
...            ...                 ...  ...                  ...               ...
307503      456247                   0  ...          5685.241231       5685.241231
307505      456249                   0  ...         22771.410000      22771.410000
307506      456251                   0  ...          7492.924286       7492.924286
307507      456252                   0  ...         10069.867500      10069.867500
307509      456254                   0  ...         10239.832895      10239.832895

[246009 rows x 331 columns]
0         1
1         0
2         0
3         0
4         0
         ..
307503    0
307505    0
307506    0
307507    0
307509    1
Name: TARGET, Length: 246009, dtype: int64
Counter({0: 226149, 1: 19860})
Counter({0: 226149, 1: 45229})
Counter({0: 64612, 1: 45229})
[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current 
value: num_threads=-1
Training until validation scores don't improve for 100 rounds
[100]   training's binary_logloss: 0.55739      valid_1's binary_logloss: 0.475477
[200]   training's binary_logloss: 0.526541     valid_1's binary_logloss: 0.459373
[300]   training's binary_logloss: 0.506909     valid_1's binary_logloss: 0.451336
[400]   training's binary_logloss: 0.49147      valid_1's binary_logloss: 0.446207
[500]   training's binary_logloss: 0.477894     valid_1's binary_logloss: 0.441991
[600]   training's binary_logloss: 0.465766     valid_1's binary_logloss: 0.438295
[700]   training's binary_logloss: 0.454435     valid_1's binary_logloss: 0.43477
[800]   training's binary_logloss: 0.443886     valid_1's binary_logloss: 0.431719
[900]   training's binary_logloss: 0.433542     valid_1's binary_logloss: 0.428801
[1000]  training's binary_logloss: 0.423651     valid_1's binary_logloss: 0.425938
[1100]  training's binary_logloss: 0.414236     valid_1's binary_logloss: 0.423394
[1200]  training's binary_logloss: 0.405425     valid_1's binary_logloss: 0.421057
[1300]  training's binary_logloss: 0.396584     valid_1's binary_logloss: 0.418592
[1400]  training's binary_logloss: 0.388384     valid_1's binary_logloss: 0.416274
[1500]  training's binary_logloss: 0.380651     valid_1's binary_logloss: 0.414105
[1600]  training's binary_logloss: 0.373064     valid_1's binary_logloss: 0.411891
[1700]  training's binary_logloss: 0.365794     valid_1's binary_logloss: 0.40975
[1800]  training's binary_logloss: 0.358639     valid_1's binary_logloss: 0.407706
[1900]  training's binary_logloss: 0.351379     valid_1's binary_logloss: 0.405667
[2000]  training's binary_logloss: 0.34468      valid_1's binary_logloss: 0.403759
[2100]  training's binary_logloss: 0.338459     valid_1's binary_logloss: 0.402015
[2200]  training's binary_logloss: 0.332232     valid_1's binary_logloss: 0.40036
[2300]  training's binary_logloss: 0.326172     valid_1's binary_logloss: 0.398653
[2400]  training's binary_logloss: 0.320287     valid_1's binary_logloss: 0.396993
[2500]  training's binary_logloss: 0.314338     valid_1's binary_logloss: 0.395317
[2600]  training's binary_logloss: 0.308598     valid_1's binary_logloss: 0.393798
[2700]  training's binary_logloss: 0.303147     valid_1's binary_logloss: 0.392308
[2800]  training's binary_logloss: 0.297538     valid_1's binary_logloss: 0.390718
[2900]  training's binary_logloss: 0.292177     valid_1's binary_logloss: 0.389278
[3000]  training's binary_logloss: 0.286989     valid_1's binary_logloss: 0.387943
[3100]  training's binary_logloss: 0.281962     valid_1's binary_logloss: 0.386514
[3200]  training's binary_logloss: 0.276963     valid_1's binary_logloss: 0.385112
[3300]  training's binary_logloss: 0.272372     valid_1's binary_logloss: 0.383949
[3400]  training's binary_logloss: 0.267698     valid_1's binary_logloss: 0.382794
[3500]  training's binary_logloss: 0.263066     valid_1's binary_logloss: 0.381594
[3600]  training's binary_logloss: 0.258623     valid_1's binary_logloss: 0.380453
[3700]  training's binary_logloss: 0.254349     valid_1's binary_logloss: 0.379378
[3800]  training's binary_logloss: 0.250068     valid_1's binary_logloss: 0.378242
[3900]  training's binary_logloss: 0.245851     valid_1's binary_logloss: 0.377228
[4000]  training's binary_logloss: 0.241734     valid_1's binary_logloss: 0.376167
[4100]  training's binary_logloss: 0.237463     valid_1's binary_logloss: 0.375129
[4200]  training's binary_logloss: 0.233408     valid_1's binary_logloss: 0.374052
[4300]  training's binary_logloss: 0.229375     valid_1's binary_logloss: 0.373002
[4400]  training's binary_logloss: 0.225614     valid_1's binary_logloss: 0.371969
[4500]  training's binary_logloss: 0.221979     valid_1's binary_logloss: 0.371022
[4600]  training's binary_logloss: 0.218664     valid_1's binary_logloss: 0.37024
[4700]  training's binary_logloss: 0.215201     valid_1's binary_logloss: 0.369291
[4800]  training's binary_logloss: 0.211875     valid_1's binary_logloss: 0.36855
[4900]  training's binary_logloss: 0.20847      valid_1's binary_logloss: 0.3677
[5000]  training's binary_logloss: 0.20532      valid_1's binary_logloss: 0.366854
[5100]  training's binary_logloss: 0.202214     valid_1's binary_logloss: 0.366142
[5200]  training's binary_logloss: 0.199075     valid_1's binary_logloss: 0.365256
[5300]  training's binary_logloss: 0.196032     valid_1's binary_logloss: 0.364561
[5400]  training's binary_logloss: 0.193066     valid_1's binary_logloss: 0.363944
[5500]  training's binary_logloss: 0.190039     valid_1's binary_logloss: 0.363229
[5600]  training's binary_logloss: 0.187163     valid_1's binary_logloss: 0.362608
[5700]  training's binary_logloss: 0.184315     valid_1's binary_logloss: 0.361944
[5800]  training's binary_logloss: 0.181678     valid_1's binary_logloss: 0.361368
[5900]  training's binary_logloss: 0.179156     valid_1's binary_logloss: 0.360746
[6000]  training's binary_logloss: 0.176382     valid_1's binary_logloss: 0.360049
[6100]  training's binary_logloss: 0.173584     valid_1's binary_logloss: 0.359288
[6200]  training's binary_logloss: 0.170937     valid_1's binary_logloss: 0.358686
[6300]  training's binary_logloss: 0.168235     valid_1's binary_logloss: 0.358078
[6400]  training's binary_logloss: 0.165818     valid_1's binary_logloss: 0.35758
[6500]  training's binary_logloss: 0.163478     valid_1's binary_logloss: 0.357044
[6600]  training's binary_logloss: 0.161292     valid_1's binary_logloss: 0.356658
[6700]  training's binary_logloss: 0.158961     valid_1's binary_logloss: 0.35614
[6800]  training's binary_logloss: 0.156792     valid_1's binary_logloss: 0.35564
[6900]  training's binary_logloss: 0.154479     valid_1's binary_logloss: 0.355114
[7000]  training's binary_logloss: 0.152235     valid_1's binary_logloss: 0.354681
[7100]  training's binary_logloss: 0.150026     valid_1's binary_logloss: 0.354199
[7200]  training's binary_logloss: 0.147893     valid_1's binary_logloss: 0.353822
[7300]  training's binary_logloss: 0.145871     valid_1's binary_logloss: 0.35335
[7400]  training's binary_logloss: 0.143949     valid_1's binary_logloss: 0.352908
[7500]  training's binary_logloss: 0.141788     valid_1's binary_logloss: 0.352355
[7600]  training's binary_logloss: 0.13976      valid_1's binary_logloss: 0.352051
[7700]  training's binary_logloss: 0.137869     valid_1's binary_logloss: 0.351699
[7800]  training's binary_logloss: 0.135903     valid_1's binary_logloss: 0.351322
[7900]  training's binary_logloss: 0.134014     valid_1's binary_logloss: 0.350975
[8000]  training's binary_logloss: 0.13204      valid_1's binary_logloss: 0.350598
[8100]  training's binary_logloss: 0.130402     valid_1's binary_logloss: 0.350296
[8200]  training's binary_logloss: 0.128663     valid_1's binary_logloss: 0.349967
[8300]  training's binary_logloss: 0.126886     valid_1's binary_logloss: 0.34969
[8400]  training's binary_logloss: 0.12518      valid_1's binary_logloss: 0.349319
[8500]  training's binary_logloss: 0.12359      valid_1's binary_logloss: 0.349007
[8600]  training's binary_logloss: 0.121897     valid_1's binary_logloss: 0.348787
[8700]  training's binary_logloss: 0.120302     valid_1's binary_logloss: 0.348568
[8800]  training's binary_logloss: 0.118792     valid_1's binary_logloss: 0.34838
[8900]  training's binary_logloss: 0.117279     valid_1's binary_logloss: 0.348133
[9000]  training's binary_logloss: 0.115665     valid_1's binary_logloss: 0.347847
[9100]  training's binary_logloss: 0.114119     valid_1's binary_logloss: 0.34761
[9200]  training's binary_logloss: 0.11262      valid_1's binary_logloss: 0.34742
[9300]  training's binary_logloss: 0.111156     valid_1's binary_logloss: 0.347171
[9400]  training's binary_logloss: 0.109888     valid_1's binary_logloss: 0.347003
[9500]  training's binary_logloss: 0.108605     valid_1's binary_logloss: 0.346883
[9600]  training's binary_logloss: 0.107074     valid_1's binary_logloss: 0.346673
[9700]  training's binary_logloss: 0.10573      valid_1's binary_logloss: 0.346466
[9800]  training's binary_logloss: 0.10432      valid_1's binary_logloss: 0.346355
[9900]  training's binary_logloss: 0.102984     valid_1's binary_logloss: 0.346191
[10000] training's binary_logloss: 0.101732     valid_1's binary_logloss: 0.346015
Did not meet early stopping. Best iteration is:
[10000] training's binary_logloss: 0.101732     valid_1's binary_logloss: 0.346015
Fold  2 AUC : 0.757507
(307511,)
Accuracy: 0.8546876524340672
F1 score: 0.3111076851923225
Recall: 0.4064451158106747
Precision: 0.251998001998002
>Train: 0=189399, 1=16633, Test: 0=93287, 1=8192
(101479,)
Accuracy: 0.9134500734142039
F1 score: 0.6180141782281564
Recall: 0.8673095703125
Precision: 0.4800351327612999
        SK_ID_CURR  NAME_CONTRACT_TYPE  ...  inst_AMT_INSTALMENT  inst_AMT_PAYMENT
0           100002                   0  ...         11559.247105      11559.247105
1           100003                   0  ...         64754.586000      64754.586000
2           100004                   1  ...          7096.155000       7096.155000
3           100006                   0  ...         62947.088438      62947.088438
4           100007                   0  ...         12666.444545      12214.060227
...            ...                 ...  ...                  ...               ...
307504      456248                   0  ...         44619.088696      43887.146087
307507      456252                   0  ...         10069.867500      10069.867500
307508      456253                   0  ...          4399.707857       4115.915357
307509      456254                   0  ...         10239.832895      10239.832895
307510      456255                   0  ...         41464.713649      47646.215878

[246009 rows x 331 columns]
0         1
1         0
2         0
3         0
4         0
         ..
307504    0
307507    0
307508    0
307509    1
307510    0
Name: TARGET, Length: 246009, dtype: int64
Counter({0: 226149, 1: 19860})
Counter({0: 226149, 1: 45229})
Counter({0: 64612, 1: 45229})
[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current 
value: num_threads=-1
Training until validation scores don't improve for 100 rounds
[100]   training's binary_logloss: 0.560247     valid_1's binary_logloss: 0.478005
[200]   training's binary_logloss: 0.529634     valid_1's binary_logloss: 0.462299
[300]   training's binary_logloss: 0.510119     valid_1's binary_logloss: 0.454547
[400]   training's binary_logloss: 0.494721     valid_1's binary_logloss: 0.449242
[500]   training's binary_logloss: 0.481482     valid_1's binary_logloss: 0.445021
[600]   training's binary_logloss: 0.469138     valid_1's binary_logloss: 0.441295
[700]   training's binary_logloss: 0.457324     valid_1's binary_logloss: 0.437705
[800]   training's binary_logloss: 0.446657     valid_1's binary_logloss: 0.434631
[900]   training's binary_logloss: 0.436629     valid_1's binary_logloss: 0.431868
[1000]  training's binary_logloss: 0.426726     valid_1's binary_logloss: 0.42909
[1100]  training's binary_logloss: 0.417377     valid_1's binary_logloss: 0.426452
[1200]  training's binary_logloss: 0.408602     valid_1's binary_logloss: 0.424064
[1300]  training's binary_logloss: 0.399779     valid_1's binary_logloss: 0.421589
[1400]  training's binary_logloss: 0.391231     valid_1's binary_logloss: 0.419022
[1500]  training's binary_logloss: 0.383471     valid_1's binary_logloss: 0.416889
[1600]  training's binary_logloss: 0.375987     valid_1's binary_logloss: 0.414788
[1700]  training's binary_logloss: 0.368605     valid_1's binary_logloss: 0.412774
[1800]  training's binary_logloss: 0.361296     valid_1's binary_logloss: 0.410671
[1900]  training's binary_logloss: 0.353996     valid_1's binary_logloss: 0.408635
[2000]  training's binary_logloss: 0.347269     valid_1's binary_logloss: 0.406737
[2100]  training's binary_logloss: 0.340815     valid_1's binary_logloss: 0.404875
[2200]  training's binary_logloss: 0.33443      valid_1's binary_logloss: 0.403092
[2300]  training's binary_logloss: 0.327934     valid_1's binary_logloss: 0.401258
[2400]  training's binary_logloss: 0.32251      valid_1's binary_logloss: 0.399771
[2500]  training's binary_logloss: 0.31687      valid_1's binary_logloss: 0.398249
[2600]  training's binary_logloss: 0.310928     valid_1's binary_logloss: 0.396521
[2700]  training's binary_logloss: 0.305333     valid_1's binary_logloss: 0.394982
[2800]  training's binary_logloss: 0.299925     valid_1's binary_logloss: 0.393572
[2900]  training's binary_logloss: 0.294489     valid_1's binary_logloss: 0.39206
[3000]  training's binary_logloss: 0.289198     valid_1's binary_logloss: 0.390626
[3100]  training's binary_logloss: 0.284403     valid_1's binary_logloss: 0.389349
[3200]  training's binary_logloss: 0.279531     valid_1's binary_logloss: 0.38799
[3300]  training's binary_logloss: 0.274813     valid_1's binary_logloss: 0.386813
[3400]  training's binary_logloss: 0.270164     valid_1's binary_logloss: 0.38553
[3500]  training's binary_logloss: 0.26588      valid_1's binary_logloss: 0.384456
[3600]  training's binary_logloss: 0.261503     valid_1's binary_logloss: 0.383311
[3700]  training's binary_logloss: 0.257244     valid_1's binary_logloss: 0.38222
[3800]  training's binary_logloss: 0.253393     valid_1's binary_logloss: 0.38131
[3900]  training's binary_logloss: 0.249293     valid_1's binary_logloss: 0.380254
[4000]  training's binary_logloss: 0.245275     valid_1's binary_logloss: 0.379249
[4100]  training's binary_logloss: 0.241141     valid_1's binary_logloss: 0.37822
[4200]  training's binary_logloss: 0.237166     valid_1's binary_logloss: 0.37713
[4300]  training's binary_logloss: 0.233249     valid_1's binary_logloss: 0.376225
[4400]  training's binary_logloss: 0.229563     valid_1's binary_logloss: 0.37524
[4500]  training's binary_logloss: 0.225987     valid_1's binary_logloss: 0.374399
[4600]  training's binary_logloss: 0.222361     valid_1's binary_logloss: 0.373561
[4700]  training's binary_logloss: 0.218807     valid_1's binary_logloss: 0.372747
[4800]  training's binary_logloss: 0.215316     valid_1's binary_logloss: 0.371923
[4900]  training's binary_logloss: 0.211979     valid_1's binary_logloss: 0.371136
[5000]  training's binary_logloss: 0.208749     valid_1's binary_logloss: 0.370367
[5100]  training's binary_logloss: 0.205516     valid_1's binary_logloss: 0.369643
[5200]  training's binary_logloss: 0.202503     valid_1's binary_logloss: 0.368961
[5300]  training's binary_logloss: 0.19935      valid_1's binary_logloss: 0.368081
[5400]  training's binary_logloss: 0.196272     valid_1's binary_logloss: 0.367308
[5500]  training's binary_logloss: 0.193279     valid_1's binary_logloss: 0.366603
[5600]  training's binary_logloss: 0.19042      valid_1's binary_logloss: 0.365977
[5700]  training's binary_logloss: 0.187679     valid_1's binary_logloss: 0.365395
[5800]  training's binary_logloss: 0.184816     valid_1's binary_logloss: 0.36468
[5900]  training's binary_logloss: 0.181983     valid_1's binary_logloss: 0.364038
[6000]  training's binary_logloss: 0.179139     valid_1's binary_logloss: 0.363406
[6100]  training's binary_logloss: 0.176291     valid_1's binary_logloss: 0.362839
[6200]  training's binary_logloss: 0.173604     valid_1's binary_logloss: 0.36226
[6300]  training's binary_logloss: 0.170964     valid_1's binary_logloss: 0.361673
[6400]  training's binary_logloss: 0.168497     valid_1's binary_logloss: 0.361182
[6500]  training's binary_logloss: 0.16595      valid_1's binary_logloss: 0.360705
[6600]  training's binary_logloss: 0.163434     valid_1's binary_logloss: 0.360135
[6700]  training's binary_logloss: 0.160962     valid_1's binary_logloss: 0.359569
[6800]  training's binary_logloss: 0.158717     valid_1's binary_logloss: 0.359141
[6900]  training's binary_logloss: 0.156653     valid_1's binary_logloss: 0.358754
[7000]  training's binary_logloss: 0.154374     valid_1's binary_logloss: 0.358247
[7100]  training's binary_logloss: 0.152158     valid_1's binary_logloss: 0.357729
[7200]  training's binary_logloss: 0.150133     valid_1's binary_logloss: 0.357299
[7300]  training's binary_logloss: 0.147963     valid_1's binary_logloss: 0.3569
[7400]  training's binary_logloss: 0.145732     valid_1's binary_logloss: 0.35642
[7500]  training's binary_logloss: 0.143609     valid_1's binary_logloss: 0.355959
[7600]  training's binary_logloss: 0.141625     valid_1's binary_logloss: 0.355562
[7700]  training's binary_logloss: 0.139697     valid_1's binary_logloss: 0.355222
[7800]  training's binary_logloss: 0.137698     valid_1's binary_logloss: 0.354889
[7900]  training's binary_logloss: 0.135979     valid_1's binary_logloss: 0.35456
[8000]  training's binary_logloss: 0.13441      valid_1's binary_logloss: 0.354308
[8100]  training's binary_logloss: 0.132603     valid_1's binary_logloss: 0.353986
[8200]  training's binary_logloss: 0.130908     valid_1's binary_logloss: 0.353696
[8300]  training's binary_logloss: 0.129118     valid_1's binary_logloss: 0.353452
[8400]  training's binary_logloss: 0.127401     valid_1's binary_logloss: 0.353216
[8500]  training's binary_logloss: 0.125607     valid_1's binary_logloss: 0.352877
[8600]  training's binary_logloss: 0.12385      valid_1's binary_logloss: 0.352666
[8700]  training's binary_logloss: 0.122258     valid_1's binary_logloss: 0.352496
[8800]  training's binary_logloss: 0.12066      valid_1's binary_logloss: 0.352215
[8900]  training's binary_logloss: 0.119014     valid_1's binary_logloss: 0.35191
[9000]  training's binary_logloss: 0.117605     valid_1's binary_logloss: 0.351708
[9100]  training's binary_logloss: 0.116164     valid_1's binary_logloss: 0.351569
[9200]  training's binary_logloss: 0.114677     valid_1's binary_logloss: 0.351369
[9300]  training's binary_logloss: 0.11327      valid_1's binary_logloss: 0.351217
[9400]  training's binary_logloss: 0.111798     valid_1's binary_logloss: 0.350925
[9500]  training's binary_logloss: 0.110214     valid_1's binary_logloss: 0.35067
[9600]  training's binary_logloss: 0.108803     valid_1's binary_logloss: 0.350442
[9700]  training's binary_logloss: 0.107397     valid_1's binary_logloss: 0.350243
[9800]  training's binary_logloss: 0.106082     valid_1's binary_logloss: 0.350092
[9900]  training's binary_logloss: 0.104652     valid_1's binary_logloss: 0.349917
[10000] training's binary_logloss: 0.103544     valid_1's binary_logloss: 0.349783
Did not meet early stopping. Best iteration is:
[10000] training's binary_logloss: 0.103544     valid_1's binary_logloss: 0.349783
Fold  3 AUC : 0.758953
(307511,)
Accuracy: 0.852882833078599
F1 score: 0.3073036288470372
Recall: 0.4042296072507553
Precision: 0.2478695813264172
>Train: 0=189399, 1=16633, Test: 0=93287, 1=8192
(101479,)
Accuracy: 0.9151942766483706
F1 score: 0.6243889664804471
Recall: 0.8731689453125
Precision: 0.4859375
        SK_ID_CURR  NAME_CONTRACT_TYPE  ...  inst_AMT_INSTALMENT  inst_AMT_PAYMENT
3           100006                   0  ...         62947.088438      62947.088438
5           100008                   0  ...         27702.964286      27360.502714
6           100009                   0  ...          9568.531765       9568.531765
7           100010                   0  ...         27449.208000      27449.208000
8           100011                   0  ...         13575.715615      11328.893654
...            ...                 ...  ...                  ...               ...
307504      456248                   0  ...         44619.088696      43887.146087
307505      456249                   0  ...         22771.410000      22771.410000
307506      456251                   0  ...          7492.924286       7492.924286
307508      456253                   0  ...          4399.707857       4115.915357
307510      456255                   0  ...         41464.713649      47646.215878

[246009 rows x 331 columns]
3         0
5         0
6         0
7         0
8         0
         ..
307504    0
307505    0
307506    0
307508    0
307510    0
Name: TARGET, Length: 246009, dtype: int64
Counter({0: 226149, 1: 19860})
Counter({0: 226149, 1: 45229})
Counter({0: 64612, 1: 45229})
[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current 
value: num_threads=-1
Training until validation scores don't improve for 100 rounds
[100]   training's binary_logloss: 0.559889     valid_1's binary_logloss: 0.477335
[200]   training's binary_logloss: 0.529488     valid_1's binary_logloss: 0.462073
[300]   training's binary_logloss: 0.509599     valid_1's binary_logloss: 0.454351
[400]   training's binary_logloss: 0.49378      valid_1's binary_logloss: 0.4491
[500]   training's binary_logloss: 0.480134     valid_1's binary_logloss: 0.445043
[600]   training's binary_logloss: 0.467779     valid_1's binary_logloss: 0.441397
[700]   training's binary_logloss: 0.456316     valid_1's binary_logloss: 0.437941
[800]   training's binary_logloss: 0.445384     valid_1's binary_logloss: 0.434842
[900]   training's binary_logloss: 0.435353     valid_1's binary_logloss: 0.431952
[1000]  training's binary_logloss: 0.425655     valid_1's binary_logloss: 0.429218
[1100]  training's binary_logloss: 0.41664      valid_1's binary_logloss: 0.426656
[1200]  training's binary_logloss: 0.407937     valid_1's binary_logloss: 0.424184
[1300]  training's binary_logloss: 0.39921      valid_1's binary_logloss: 0.421789
[1400]  training's binary_logloss: 0.390858     valid_1's binary_logloss: 0.419359
[1500]  training's binary_logloss: 0.383086     valid_1's binary_logloss: 0.417164
[1600]  training's binary_logloss: 0.375169     valid_1's binary_logloss: 0.41483
[1700]  training's binary_logloss: 0.3677       valid_1's binary_logloss: 0.412667
[1800]  training's binary_logloss: 0.360678     valid_1's binary_logloss: 0.410742
[1900]  training's binary_logloss: 0.353714     valid_1's binary_logloss: 0.40885
[2000]  training's binary_logloss: 0.346857     valid_1's binary_logloss: 0.406898
[2100]  training's binary_logloss: 0.339755     valid_1's binary_logloss: 0.404846
[2200]  training's binary_logloss: 0.333242     valid_1's binary_logloss: 0.402988
[2300]  training's binary_logloss: 0.326978     valid_1's binary_logloss: 0.401254
[2400]  training's binary_logloss: 0.320869     valid_1's binary_logloss: 0.39952
[2500]  training's binary_logloss: 0.315232     valid_1's binary_logloss: 0.397996
[2600]  training's binary_logloss: 0.309664     valid_1's binary_logloss: 0.3964
[2700]  training's binary_logloss: 0.303863     valid_1's binary_logloss: 0.394692
[2800]  training's binary_logloss: 0.29831      valid_1's binary_logloss: 0.393164
[2900]  training's binary_logloss: 0.292819     valid_1's binary_logloss: 0.391702
[3000]  training's binary_logloss: 0.287633     valid_1's binary_logloss: 0.390292
[3100]  training's binary_logloss: 0.282422     valid_1's binary_logloss: 0.3889
[3200]  training's binary_logloss: 0.277567     valid_1's binary_logloss: 0.387655
[3300]  training's binary_logloss: 0.272763     valid_1's binary_logloss: 0.386411
[3400]  training's binary_logloss: 0.268035     valid_1's binary_logloss: 0.385246
[3500]  training's binary_logloss: 0.263118     valid_1's binary_logloss: 0.383969
[3600]  training's binary_logloss: 0.258536     valid_1's binary_logloss: 0.382664
[3700]  training's binary_logloss: 0.254021     valid_1's binary_logloss: 0.381359
[3800]  training's binary_logloss: 0.249836     valid_1's binary_logloss: 0.38023
[3900]  training's binary_logloss: 0.245537     valid_1's binary_logloss: 0.379028
[4000]  training's binary_logloss: 0.241379     valid_1's binary_logloss: 0.377916
[4100]  training's binary_logloss: 0.237369     valid_1's binary_logloss: 0.37689
[4200]  training's binary_logloss: 0.233393     valid_1's binary_logloss: 0.375872
[4300]  training's binary_logloss: 0.229488     valid_1's binary_logloss: 0.374825
[4400]  training's binary_logloss: 0.225698     valid_1's binary_logloss: 0.373899
[4500]  training's binary_logloss: 0.22199      valid_1's binary_logloss: 0.372877
[4600]  training's binary_logloss: 0.218324     valid_1's binary_logloss: 0.371914
[4700]  training's binary_logloss: 0.214726     valid_1's binary_logloss: 0.371032
[4800]  training's binary_logloss: 0.211441     valid_1's binary_logloss: 0.37024
[4900]  training's binary_logloss: 0.208285     valid_1's binary_logloss: 0.369458
[5000]  training's binary_logloss: 0.205006     valid_1's binary_logloss: 0.368701
[5100]  training's binary_logloss: 0.202003     valid_1's binary_logloss: 0.367961
[5200]  training's binary_logloss: 0.198911     valid_1's binary_logloss: 0.367288
[5300]  training's binary_logloss: 0.195881     valid_1's binary_logloss: 0.366592
[5400]  training's binary_logloss: 0.192785     valid_1's binary_logloss: 0.365828
[5500]  training's binary_logloss: 0.189918     valid_1's binary_logloss: 0.365168
[5600]  training's binary_logloss: 0.187064     valid_1's binary_logloss: 0.364469
[5700]  training's binary_logloss: 0.184254     valid_1's binary_logloss: 0.363802
[5800]  training's binary_logloss: 0.181373     valid_1's binary_logloss: 0.36313
[5900]  training's binary_logloss: 0.178494     valid_1's binary_logloss: 0.362386
[6000]  training's binary_logloss: 0.175757     valid_1's binary_logloss: 0.361708
[6100]  training's binary_logloss: 0.172988     valid_1's binary_logloss: 0.361033
[6200]  training's binary_logloss: 0.170392     valid_1's binary_logloss: 0.360341
[6300]  training's binary_logloss: 0.167938     valid_1's binary_logloss: 0.359768
[6400]  training's binary_logloss: 0.165413     valid_1's binary_logloss: 0.359206
[6500]  training's binary_logloss: 0.162949     valid_1's binary_logloss: 0.358761
[6600]  training's binary_logloss: 0.160538     valid_1's binary_logloss: 0.358284
[6700]  training's binary_logloss: 0.158251     valid_1's binary_logloss: 0.357876
[6800]  training's binary_logloss: 0.155862     valid_1's binary_logloss: 0.357388
[6900]  training's binary_logloss: 0.153597     valid_1's binary_logloss: 0.356964
[7000]  training's binary_logloss: 0.151281     valid_1's binary_logloss: 0.356521
[7100]  training's binary_logloss: 0.148964     valid_1's binary_logloss: 0.356
[7200]  training's binary_logloss: 0.14676      valid_1's binary_logloss: 0.355547
[7300]  training's binary_logloss: 0.144603     valid_1's binary_logloss: 0.355114
[7400]  training's binary_logloss: 0.142568     valid_1's binary_logloss: 0.354721
[7500]  training's binary_logloss: 0.14054      valid_1's binary_logloss: 0.354399
[7600]  training's binary_logloss: 0.138633     valid_1's binary_logloss: 0.353974
[7700]  training's binary_logloss: 0.136628     valid_1's binary_logloss: 0.353625
[7800]  training's binary_logloss: 0.134778     valid_1's binary_logloss: 0.35337
[7900]  training's binary_logloss: 0.132957     valid_1's binary_logloss: 0.353059
[8000]  training's binary_logloss: 0.131196     valid_1's binary_logloss: 0.352686
[8100]  training's binary_logloss: 0.129462     valid_1's binary_logloss: 0.352409
[8200]  training's binary_logloss: 0.1277       valid_1's binary_logloss: 0.352092
[8300]  training's binary_logloss: 0.125879     valid_1's binary_logloss: 0.351782
[8400]  training's binary_logloss: 0.124119     valid_1's binary_logloss: 0.351538
[8500]  training's binary_logloss: 0.122376     valid_1's binary_logloss: 0.351188
[8600]  training's binary_logloss: 0.120681     valid_1's binary_logloss: 0.350903
[8700]  training's binary_logloss: 0.119074     valid_1's binary_logloss: 0.350718
[8800]  training's binary_logloss: 0.117378     valid_1's binary_logloss: 0.350436
[8900]  training's binary_logloss: 0.115845     valid_1's binary_logloss: 0.350234
[9000]  training's binary_logloss: 0.114306     valid_1's binary_logloss: 0.349989
[9100]  training's binary_logloss: 0.1129       valid_1's binary_logloss: 0.349697
[9200]  training's binary_logloss: 0.111351     valid_1's binary_logloss: 0.349486
[9300]  training's binary_logloss: 0.109974     valid_1's binary_logloss: 0.349294
[9400]  training's binary_logloss: 0.108491     valid_1's binary_logloss: 0.349127
[9500]  training's binary_logloss: 0.107166     valid_1's binary_logloss: 0.348892
[9600]  training's binary_logloss: 0.105807     valid_1's binary_logloss: 0.348702
[9700]  training's binary_logloss: 0.104408     valid_1's binary_logloss: 0.348474
[9800]  training's binary_logloss: 0.103136     valid_1's binary_logloss: 0.348336
[9900]  training's binary_logloss: 0.101676     valid_1's binary_logloss: 0.348117
[10000] training's binary_logloss: 0.100417     valid_1's binary_logloss: 0.347975
Did not meet early stopping. Best iteration is:
[10000] training's binary_logloss: 0.100417     valid_1's binary_logloss: 0.347975
Fold  4 AUC : 0.762119
(307511,)
Accuracy: 0.8529966505154304
F1 score: 0.3114766582895438
Recall: 0.4118831822759315
Precision: 0.25042860641685033
>Train: 0=189399, 1=16633, Test: 0=93287, 1=8192
(101479,)
Accuracy: 0.9136570127809694
F1 score: 0.6191759388038943
Recall: 0.8695068359375
Precision: 0.48076403887688984
        SK_ID_CURR  NAME_CONTRACT_TYPE  ...  inst_AMT_INSTALMENT  inst_AMT_PAYMENT
0           100002                   0  ...         11559.247105      11559.247105
1           100003                   0  ...         64754.586000      64754.586000
2           100004                   1  ...          7096.155000       7096.155000
4           100007                   0  ...         12666.444545      12214.060227
5           100008                   0  ...         27702.964286      27360.502714
...            ...                 ...  ...                  ...               ...
307506      456251                   0  ...          7492.924286       7492.924286
307507      456252                   0  ...         10069.867500      10069.867500
307508      456253                   0  ...          4399.707857       4115.915357
307509      456254                   0  ...         10239.832895      10239.832895
307510      456255                   0  ...         41464.713649      47646.215878

[246009 rows x 331 columns]
0         1
1         0
2         0
4         0
5         0
         ..
307506    0
307507    0
307508    0
307509    1
307510    0
Name: TARGET, Length: 246009, dtype: int64
Counter({0: 226149, 1: 19860})
Counter({0: 226149, 1: 45229})
Counter({0: 64612, 1: 45229})
[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current 
value: num_threads=-1
Training until validation scores don't improve for 100 rounds
[100]   training's binary_logloss: 0.559678     valid_1's binary_logloss: 0.476519
[200]   training's binary_logloss: 0.528152     valid_1's binary_logloss: 0.460039
[300]   training's binary_logloss: 0.508134     valid_1's binary_logloss: 0.451859
[400]   training's binary_logloss: 0.492588     valid_1's binary_logloss: 0.446655
[500]   training's binary_logloss: 0.479072     valid_1's binary_logloss: 0.442441
[600]   training's binary_logloss: 0.466415     valid_1's binary_logloss: 0.43843
[700]   training's binary_logloss: 0.455016     valid_1's binary_logloss: 0.43512
[800]   training's binary_logloss: 0.444206     valid_1's binary_logloss: 0.431939
[900]   training's binary_logloss: 0.433992     valid_1's binary_logloss: 0.429019
[1000]  training's binary_logloss: 0.424342     valid_1's binary_logloss: 0.426126
[1100]  training's binary_logloss: 0.414893     valid_1's binary_logloss: 0.423349
[1200]  training's binary_logloss: 0.406052     valid_1's binary_logloss: 0.420738
[1300]  training's binary_logloss: 0.397662     valid_1's binary_logloss: 0.418329
[1400]  training's binary_logloss: 0.389288     valid_1's binary_logloss: 0.415811
[1500]  training's binary_logloss: 0.381684     valid_1's binary_logloss: 0.413706
[1600]  training's binary_logloss: 0.373801     valid_1's binary_logloss: 0.411496
[1700]  training's binary_logloss: 0.365899     valid_1's binary_logloss: 0.409234
[1800]  training's binary_logloss: 0.358504     valid_1's binary_logloss: 0.407229
[1900]  training's binary_logloss: 0.351635     valid_1's binary_logloss: 0.405208
[2000]  training's binary_logloss: 0.345151     valid_1's binary_logloss: 0.40339
[2100]  training's binary_logloss: 0.33881      valid_1's binary_logloss: 0.401588
[2200]  training's binary_logloss: 0.33262      valid_1's binary_logloss: 0.399856
[2300]  training's binary_logloss: 0.326448     valid_1's binary_logloss: 0.398254
[2400]  training's binary_logloss: 0.320458     valid_1's binary_logloss: 0.396601
[2500]  training's binary_logloss: 0.314582     valid_1's binary_logloss: 0.394987
[2600]  training's binary_logloss: 0.309164     valid_1's binary_logloss: 0.393407
[2700]  training's binary_logloss: 0.303736     valid_1's binary_logloss: 0.391944
[2800]  training's binary_logloss: 0.298595     valid_1's binary_logloss: 0.3905
[2900]  training's binary_logloss: 0.293141     valid_1's binary_logloss: 0.389021
[3000]  training's binary_logloss: 0.288109     valid_1's binary_logloss: 0.387634
[3100]  training's binary_logloss: 0.28322      valid_1's binary_logloss: 0.386333
[3200]  training's binary_logloss: 0.278312     valid_1's binary_logloss: 0.385065
[3300]  training's binary_logloss: 0.273586     valid_1's binary_logloss: 0.383829
[3400]  training's binary_logloss: 0.268791     valid_1's binary_logloss: 0.382595
[3500]  training's binary_logloss: 0.264569     valid_1's binary_logloss: 0.381498
[3600]  training's binary_logloss: 0.260439     valid_1's binary_logloss: 0.380478
[3700]  training's binary_logloss: 0.256109     valid_1's binary_logloss: 0.379348
[3800]  training's binary_logloss: 0.251908     valid_1's binary_logloss: 0.37832
[3900]  training's binary_logloss: 0.247653     valid_1's binary_logloss: 0.377167
[4000]  training's binary_logloss: 0.243612     valid_1's binary_logloss: 0.376026
[4100]  training's binary_logloss: 0.239711     valid_1's binary_logloss: 0.375062
[4200]  training's binary_logloss: 0.235757     valid_1's binary_logloss: 0.374038
[4300]  training's binary_logloss: 0.231889     valid_1's binary_logloss: 0.373009
[4400]  training's binary_logloss: 0.228294     valid_1's binary_logloss: 0.372079
[4500]  training's binary_logloss: 0.224521     valid_1's binary_logloss: 0.371153
[4600]  training's binary_logloss: 0.221139     valid_1's binary_logloss: 0.370352
[4700]  training's binary_logloss: 0.217886     valid_1's binary_logloss: 0.369509
[4800]  training's binary_logloss: 0.214706     valid_1's binary_logloss: 0.368732
[4900]  training's binary_logloss: 0.211458     valid_1's binary_logloss: 0.367916
[5000]  training's binary_logloss: 0.208126     valid_1's binary_logloss: 0.367145
[5100]  training's binary_logloss: 0.204695     valid_1's binary_logloss: 0.366428
[5200]  training's binary_logloss: 0.20136      valid_1's binary_logloss: 0.365677
[5300]  training's binary_logloss: 0.19836      valid_1's binary_logloss: 0.364975
[5400]  training's binary_logloss: 0.19516      valid_1's binary_logloss: 0.364133
[5500]  training's binary_logloss: 0.1921       valid_1's binary_logloss: 0.363339
[5600]  training's binary_logloss: 0.189306     valid_1's binary_logloss: 0.36282
[5700]  training's binary_logloss: 0.186364     valid_1's binary_logloss: 0.362197
[5800]  training's binary_logloss: 0.183511     valid_1's binary_logloss: 0.361595
[5900]  training's binary_logloss: 0.180587     valid_1's binary_logloss: 0.360912
[6000]  training's binary_logloss: 0.177822     valid_1's binary_logloss: 0.36032
[6100]  training's binary_logloss: 0.175313     valid_1's binary_logloss: 0.3599
[6200]  training's binary_logloss: 0.172635     valid_1's binary_logloss: 0.359358
[6300]  training's binary_logloss: 0.170258     valid_1's binary_logloss: 0.358882
[6400]  training's binary_logloss: 0.168047     valid_1's binary_logloss: 0.358395
[6500]  training's binary_logloss: 0.165585     valid_1's binary_logloss: 0.357841
[6600]  training's binary_logloss: 0.163065     valid_1's binary_logloss: 0.357341
[6700]  training's binary_logloss: 0.160776     valid_1's binary_logloss: 0.356889
[6800]  training's binary_logloss: 0.158427     valid_1's binary_logloss: 0.356426
[6900]  training's binary_logloss: 0.156215     valid_1's binary_logloss: 0.355986
[7000]  training's binary_logloss: 0.153971     valid_1's binary_logloss: 0.355492
[7100]  training's binary_logloss: 0.151847     valid_1's binary_logloss: 0.355096
[7200]  training's binary_logloss: 0.149573     valid_1's binary_logloss: 0.354607
[7300]  training's binary_logloss: 0.147605     valid_1's binary_logloss: 0.354209
[7400]  training's binary_logloss: 0.145421     valid_1's binary_logloss: 0.353799
[7500]  training's binary_logloss: 0.143445     valid_1's binary_logloss: 0.353402
[7600]  training's binary_logloss: 0.14148      valid_1's binary_logloss: 0.353035
[7700]  training's binary_logloss: 0.139412     valid_1's binary_logloss: 0.352667
[7800]  training's binary_logloss: 0.137405     valid_1's binary_logloss: 0.352257
[7900]  training's binary_logloss: 0.135456     valid_1's binary_logloss: 0.351954
[8000]  training's binary_logloss: 0.13369      valid_1's binary_logloss: 0.351657
[8100]  training's binary_logloss: 0.13181      valid_1's binary_logloss: 0.351324
[8200]  training's binary_logloss: 0.129907     valid_1's binary_logloss: 0.351009
[8300]  training's binary_logloss: 0.128152     valid_1's binary_logloss: 0.350681
[8400]  training's binary_logloss: 0.126496     valid_1's binary_logloss: 0.350351
[8500]  training's binary_logloss: 0.124835     valid_1's binary_logloss: 0.350107
[8600]  training's binary_logloss: 0.123109     valid_1's binary_logloss: 0.34983
[8700]  training's binary_logloss: 0.121433     valid_1's binary_logloss: 0.349537
[8800]  training's binary_logloss: 0.119746     valid_1's binary_logloss: 0.349195
[8900]  training's binary_logloss: 0.118124     valid_1's binary_logloss: 0.348983
[9000]  training's binary_logloss: 0.116607     valid_1's binary_logloss: 0.348791
[9100]  training's binary_logloss: 0.115045     valid_1's binary_logloss: 0.348467
[9200]  training's binary_logloss: 0.11361      valid_1's binary_logloss: 0.348305
[9300]  training's binary_logloss: 0.112191     valid_1's binary_logloss: 0.348092
[9400]  training's binary_logloss: 0.110613     valid_1's binary_logloss: 0.347849
[9500]  training's binary_logloss: 0.109171     valid_1's binary_logloss: 0.347667
[9600]  training's binary_logloss: 0.107706     valid_1's binary_logloss: 0.347418
[9700]  training's binary_logloss: 0.106465     valid_1's binary_logloss: 0.347341
[9800]  training's binary_logloss: 0.105022     valid_1's binary_logloss: 0.347164
[9900]  training's binary_logloss: 0.103743     valid_1's binary_logloss: 0.347025
[10000] training's binary_logloss: 0.102499     valid_1's binary_logloss: 0.346833
Did not meet early stopping. Best iteration is:
[10000] training's binary_logloss: 0.102499     valid_1's binary_logloss: 0.346833
Fold  5 AUC : 0.758223
(307511,)
Accuracy: 0.8543136808559071
F1 score: 0.3128834355828221
Recall: 0.4108761329305136
Precision: 0.25263157894736843
>Train: 0=189399, 1=16633, Test: 0=93287, 1=8192
(101479,)
Accuracy: 0.9141004542811814
F1 score: 0.6208186523989734
Recall: 0.87109375
Precision: 0.4822599175508549
                 precision    recall   f1score       support
0             0   0.945252  0.893256  0.918519  56537.000000
1             1   0.252632  0.410876  0.312883   4965.000000
Read test data
Shape test data:  (48744, 121)
Shape data for dashboard:  (48744, 6)
Shape test data:  (48744, 331)
sampled_data_1 (307511, 3)
sampled_data_2 (307511, 3)
sampled_data_3 (307511, 333)
sampled_data (307511, 338)
test (48744, 338)
processed_data (25000, 338)